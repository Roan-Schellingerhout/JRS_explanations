\section{Related work}

%  \fb{Could we add something on the explanation types that we propose?}

Within the field of explainable AI, there is no single agreed-upon method to provide explanations \cite{arya2019one}. Different use cases require different approaches, each with their own strengths and weaknesses. 

One of the most common methods of providing explanations is through text \cite{goebel2018explainable,poli2021generation,szymanski2021visual}. Textual explanations consist of brief sections of text that explain the rationale of the XAI model. Such texts often contain information on the impact different features had on the prediction and how those features interacted with each other. There are multiple ways to generate such texts, e.g., through the use of large language models (LLMs) \cite{menon2022visual} or predefined templates \cite{wrede2022linguistic}. 

Another popular approach is the use of feature attribution maps: visualizations that show the importance of different features to the prediction \cite{palacio2021xai}. Such maps can take different forms, depending on the specific task and data involved. When using tabular data, bar charts are often used to show the contribution of each different feature type to the prediction. When using multi-dimensional data, such as images or time series, are used, heatmaps can provide an overview of the importance of the different dimensions interacting with each other \cite{fauvel2021xcm}. 

A further explanation type that has been gaining popularity recently, is the knowledge graph-based explanation \cite{tiddi2022knowledge}. These explanations depend on the connections within a knowledge graph to explain the rationale behind a prediction. This is usually done by highlighting important nodes and edges within the graph, which provide `paths' from the subject to the recommended item, accompanied by their importance to the model's prediction \cite{wang2019kgat}. 


\subsection{Challenges in multi-stakeholder explainability}
In multi-stakeholder environments, explanations need to meet additional requirements \cite{abdollahpouri2020multistakeholder}. An explanation that is sufficient for a developer, is not necessarily understandable for a user or provider, and vice versa \cite{szymanski2021visual}. There are multiple strategies to deal with this discrepancy, each with its own strengths and weaknesses. The most obvious solution is to create individual explanations for the different stakeholders \cite{yildirim2021bideepfm}. Although this leads to the most fine-tuned explanations, it introduces an additional layer of complexity to the system as a whole. Another approach would be to simply use a single explanation, but to present it differently based on the stakeholders' level of expertise \cite{abdollahpouri2020multistakeholder}. Unfortunately, it can be difficult to incorporate the different stakeholder perspectives simultaneously - some facts could be confidential or sensitive for a specific stakeholder, making it difficult to incorporate them in the explanation, even when they are relevant. Similarly, a highly specific overview of how the model came to the prediction might be useful for a developer, but will be too confusing for a lay user or provider. 

\subsection{Explainability in job recommender systems}
Explaining reciprocal recommendations, such as job recommendations, tends to be more difficult than standard recommendations, as the preferences of both parties need to be considered. Kleinerman et al. \cite{kleinerman2018providing} determined that explanations that consider both parties outperform one-sided explanations in high-cost scenarios (such as recruitment). In particular, explanations based on specific feature values are useful, although only a limited number of features should be included to prevent information overload. In high-cost scenarios, explanations should not stay limited to personal preferences (e.g. `you should apply for this job because you want a company that has X attributes'), but should also incorporate an explanation of why the other party is likely to agree (e.g. `they are likely to accept you, because they are looking for a candidate with Y skills'). 

In job recommender systems (JRSs) specifically, explainability has largely gone unexplored. While some previous work has incorporated some degree of explainability within their JRSs, the explanations are often limited and seem to have been included as an afterthought \cite{le2019towards,Upadhyay2021,yildirim2021bideepfm}. Even when explainability has been included, authors usually fail to consider all stakeholders, tailoring the explanations to developers only. One could argue that reciprocal, easy-to-understand explainability should be at the core of the models' design in a high-risk, high-impact domain such as recruitment. Where previous research mainly falls short, is in the understandability of their explanations: while their models can technically explain some part of their predictions, the explanations tend to be unintuitive and/or limited, either staying too vague \cite{le2019towards,Upadhyay2021} or being hard to understand \cite{yildirim2021bideepfm}. When dealing with users with limited AI knowledge, such as recruiters, job seekers, and most company representatives, having clear, straightforward explanations is crucial \cite{schellingerhout2022explainable,szymanski2021visual}. To accomplish this, structured requirements engineering needs to be conducted in order to determine the preferences of all stakeholders, after which explainable JRSs will need to be designed with those requirements as a starting point. 

\subsection{Determining stakeholder preferences}
In order to determine the explanation preferences of different stakeholders, their requirements, struggles, and level of expertise need to be documented. To accomplish this, multiple approaches exist; for example, whenever the preferences of a stakeholder are already largely known (e.g., through previous research) questionnaires can be used in combination with different alterations of some explanation type \cite{szymanski2021visual}. The results of these questionnaires could then be used to `fine-tune' the already-known explanation type to better fit the exact stakeholders. However, within job recommendation, stakeholder preferences (beyond candidates) are mostly unknown \cite{de2021job}. Therefore, it is better to determine the stakeholder preferences from the ground up, allowing them to assist in shaping the explanations themselves. Thus, (semi-structured) interviews are highly useful, as they give stakeholders the freedom to indicate their exact preferences and requirements regarding explanations \cite{longhurst2003semi}. 

\subsection{Contributions}
Explainability within multi-stakeholder environments has largely gone unexplored. Research that has touched upon this topic, has often stuck to offline methods of evaluation, which fall short in high-impact domains, such as recruitment and healthcare. Therefore, this paper aims to lay the foundation for future research on explainable multi-stakeholder recommendation. We do so firstly by providing a validated interview guide that can be used to extract the explanation preferences of different stakeholder types. Furthermore, we extend the current literature on explainable job recommender systems by not just focusing on a single stakeholder, but providing guidelines on how explanations should be designed for all stakeholders involved. 