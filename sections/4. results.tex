% DESCRIBE experimental variables, the explanation types, within-subject, demographic data
\section{Results}
Based on the answers given by the different stakeholder representatives, the interview guide has been updated, and the preliminary preferences of each stakeholder type have been determined. The full transcripts of each interview are available on \href{https://github.com/Roan-Schellingerhout/JRS_explanations}{GitHub}. 

\subsection{Interview guide}

The interview guide (\cref{tab:interview_guide}) has been validated based on responses from the participants. While the interview guide was largely proven to be adequate for determining the explanation preferences of different stakeholders, some changes have been made based on the feedback we received. 

Firstly, we added an additional question to the section on \textbf{usefulness}: `\textit{how could you see yourself using the explanation in your daily work/task?}' Even when some of the participants could see that an explanation was sensible, or could be helpful in making a decision, they mentioned that they would personally stick to using another approach (e.g., a different type of explanation, or doing things manually). Although this is likely to come up using the current interview guide already, we decided to also explicitly ask the question - after all, the goal of creating an explainable model is that users end up actually using the explanations to assist them in their decision-making. 

We additionally added a new question to the \textbf{correct interpretation} section of the guide: ``\textit{how would you put the model's explanation into your own words?}'' While the participants often managed to quickly identify the most/least important features and components of the explanation, that did not necessarily indicate they properly understood the entire rationale. For example, some participants correctly identified the importance of the different job types to the explanation, but they could not properly connect all the dots, causing them to be unable to view the explanation as a single whole. By explicitly asking them to define the explanation in their own words, it becomes clear whether they adequately understand the entire explanation, or are still grasping at straws. 


Lastly, we changed one of the questions in the section on \textbf{transparency}: ``can you think of anything that would further improve your understanding?'' has been changed to ``what information is missing that could allow you to get a better understanding of the model's recommendation?''. The previous phrasing of the question was too general, making it difficult for participants to answer it on the spot. By directly asking them for information that is missing, it should be easier for them to come up with an answer, albeit a more indirect one. 


The updated, validated interview guide can be seen in \cref{tab:interview_guide_updated}.

\begin{table*}[ht]
\captionsetup{width=1.5\textwidth}
\caption{The validated, updated interview guide.}
\scriptsize
\begin{adjustwidth}{-2.5in}{-2.5in}
\centering
\begin{tabularx}{1.5\textwidth}{@{}XX>{\raggedright\arraybackslash}p{5.5cm}>{\raggedright\arraybackslash}p{6.5cm}@{}}
\toprule
\textbf{Evaluation Objective} & \textbf{Objective Description} & \textbf{Questions} & \textbf{Probing questions} \\ \midrule

1. Correct interpretation     & To assess whether or not the stakeholder can correctly interpret the explanation. & \begin{enumerate} \item[1.1] What information/features do you think were most important for this prediction? \item[1.2] What was the least important? \item[1.3] How would you put the model's explanation into your own words? \end{enumerate} &         \begin{enumerate} \item[1.1.1] What did you look at to come to that conclusion? \end{enumerate} \\ \midrule

2. Transparency & To determine the explanation's effect on understanding the model's inner workings.                & \begin{enumerate} \item[2.1] Does the explanation help you comprehend why the system gave the recommendation? \end{enumerate} & \begin{enumerate} \item[2.1.1] What components help you specifically? \item[2.1.2] what information is missing that could allow you to get a better understanding of the model's recommendation \end{enumerate} \\ \midrule

3. Usefulness   & To evaluate how useful the explanations are considered to be.                    &     \begin{enumerate} \item[3.1] Does the explanation make sense to you? \item[3.2] Does the explanation help you make a decision? \item[3.3] How could you see yourself using the explanation in your daily work/task? \end{enumerate} & \begin{enumerate} \item[3.1.1] What do you consider sensible (e.g., focus on specific features)? \item[3.1.2] What do you consider insensible? \item[3.2.1] Would you prefer a model with explanations over one without? \end{enumerate} \\ \midrule

4. Trust        & To gauge the explanation's impact on the model's trustworthiness.      &     \begin{enumerate} \item[4.1] Do you think the prediction made by the model is reliable? \item[4.2] If this recommendation was made for you, would you trust the model to have made the right decision? \end{enumerate} & \begin{enumerate} \item[4.2.1] Anything specific that makes you say that (e.g., something makes no sense, or is very similar to how you look at things)? \end{enumerate} \\ \midrule

5. Preference   & To figure out the personal preference of the stakeholder.                        &     \begin{enumerate} \item[5.1] What would you like to see added to the current explanation? \item[5.2] What would you consider to be redundant within this explanation? \end{enumerate} &  \begin{enumerate} \item[5.1.1] Any specific information that is missing? \item[5.1.2] Any functionality that could be useful? \item[5.2.1] Anything that should be removed? \item[5.2.2] Or be made less prevalent?\end{enumerate}             \\ \bottomrule
\end{tabularx}
\label{tab:interview_guide_updated}
\end{adjustwidth}
\end{table*}

\subsection{Stakeholder preferences.}
Each of the transcripts has been analyzed, and the analyses have been grouped based on stakeholder type. An overview of the generated codes, categories, theories, and relevant quotes can be found in \cref{app:GT}.

\subsubsection{Candidates.}
In line with our hypothesis, the textual explanation was well-received by the candidates. Although the candidates did receive the explanations favorably, they indicated some issues that should be addressed. For one, the specific language used in the explanation made it more difficult for the candidates to parse it correctly. Candidates also sometimes wound up losing track of the essence of the explanation, specifically when multiple trains of thought were addressed in a single paragraph, or whenever points were reiterated multiple times (\textit{``it's a bit more clear, but I don't know... I still can't follow it completely. I find it very hard to read"}, P2, Q1.1). However, the candidates still managed to correctly identify the main arguments on which the recommendation was based. They did indicate that they would prefer to be able to evaluate the text at a glance, i.e., by putting crucial information clearly at the top of the text (`\textit{`Information like the city, and what the salary is, or things of the sorts, are currently not included"}, P1, Q4.2). They did not fully trust the model, but found it to be a nice `brain-storming partner', which could support them in their decision (\textit{``if I had any doubts, the explanation would take those away''}, P2, Q3.2), and provide them with some interesting vacancies to explore on their own. Furthermore, the explanation contained the full ID of candidates and vacancies relevant to the recommendation. Considering the candidates have no access to the actual database, these IDs turned out to be of little value, and actually overwhelmed the candidates. Additionally, while multiple different vacancies were mentioned in the explanation, these were not directly accessible to the candidates - something they considered to be unintuitive. 

The graph-based explanation turned out to be difficult to use for the candidates without receiving some additional help on how to interpret it. Especially the full, unsimplified version of the graph, in which all of the different paths were visible, turned out to be too overwhelming and complex to be useful (after being corrected on their interpretation: \textit{``Now I get that the thin lines were kind of like side tracks that weren't successful"}, P2, Q2.1). However, with some additional guidance on how it was structured, and by considering the simplified view of the graph, the candidates eventually correctly understood its content. Still, the amount of information contained within the graph was more confusing than helpful - for example, the types and values of the edges were constantly visible, meaning there was a lot of text present at all times. Considering the candidates did not necessarily understand the meaning of the edge types and values, they did not get any benefit from them. While the graph was indicated to give them a better understanding of the model's actual rationale, it also made them question the adequacy of the recommendation to an extent. Because vacancies different from the one being recommended were included in the graph as well, sometimes at locations seemingly `closer' to the candidate, they were unsure why \emph{those} vacancies were not recommended to them (\textit{``It feels quite strange, that the path passes through different relevant vacancies, but we just ignore those"}, P1, Q1.1). Therefore, providing candidates with information on why alternative vacancies were not recommended could be helpful. 

The current implementation of the feature attribution chart turned out to be close to useless for providing an explanation to the candidates. Since it ascribed importance to different vacancies and candidates (i.e., similar vacancies and candidates, which were included to allow for collaborative filtering \cite{su2009survey}), which they were not familiar with, it did not contribute to their understanding of the model's prediction. However, they did indicate how feature attribution could be made useful: for the bar chart to relate to their individual skills, so the candidates could understand which of their own skills were considered most important for the recommendation at a glance (\textit{``That it shows what is important for the match, for example, that your experience with Excel matters, but that your ability to be a truck driver wasn't important"}, P1, Q1.1). This would allow them to quickly verify and scrutinize recommendations. For example, if they saw a specific skill they possess was attributed a lot of importance by the model, even though they would not enjoy performing it as their job. Thus, the feature attribution chart should stick only to the personal, actionable features of the candidates. To still include previous vacancies fulfilled by the candidate, they could alternatively be grouped by job type, so that they could be represented as a single bar relating to their experience in that field (\textit{``the function types make sense to me, but the individual vacancies and candidates do not"}, P1, Q3.1). 


% \begin{itemize}
%     \item Textual explanation was well-received. However, it had some redundancy, which lowered the participants' opinions. Furthermore, the fact that multiple `arguments' were discussed seemingly interchangeably, made it more difficult to parse fully. However, the participants correctly identified the main arguments on which the recommendation was based. Furthermore, the numbers behind different candidates and vacancies were seen as confusing and overwhelming - the participants indicated that actual names or statements such as `another candidate' are preferable. Also, they indicated that it would be nice to be able to click on the vacancies/function types, and be presented with an overview of similar vacancies/different vacancies of the same type. They did not fully trust the model, but found it to be a nice `brain-storming partner'. They did indicate that it would be nice to evaluate the text at a glance, i.e., by putting crucial information clearly at the top. 
%     \item While the participants again correctly interpreted the information within the graph, it took them quite some time and required some additional hints. Once it `clicked', it was considered useful as an expansion of the textual explanations. The text and values were considered redundant, or even overwhelming. They indicated it would be nice to know why alternative vacancies were \textit{not} recommended --> \cite{kleinerman2018providing} --> good to include company-side score as well (i.e., they weren't recommended because you were unlikely to be accepted, while the recommended vacancy is likely to be mutually interested). 
    
%     \item The current implementation of the bar chart was indicated to be confusing and contributed little, or even nothing, to increasing the understanding of the explanation. However, it was indicated that it has potential, as having a sorted bar chart of, e.g., the best-matching skills and function types between the candidate and vacancy could be nice. This would, for example, allow candidates to easily spot false positives by determining that a skill they do possess, but do not consider interesting enough to use in their work, was highly influential on the prediction.
% \end{itemize}

\subsubsection{Recruiters.}
As was hypothesized, the recruiters found the textual explanations to be informative and useful. Although they found the texts to indicate some redundancy, as well as some tricky language, they considered them to be rather useful regardless. They immediately understood the main message of the text, but found that some information was reiterated too often (\textit{``It keeps beating around the bush with the same words"}, P3, Q2.1). The recruiters explicitly stated that they would not blindly trust the model, even when accompanied by a sensible explanation - they would always want to be able to manually verify its recommendation by reading the CV and vacancy text (\textit{``Very little is told about the candidate, and the vacancy, but I just have to trust that ... it would be nice to check if it's actually right"}, P3, Q2.1). However, as long as they considered that the explanation made sense, they would ``\textit{move that CV and vacancy to the top of their list}''. Furthermore, being able to quickly rule out specific candidates or vacancies was something they considered highly important. As a result, they strongly preferred to have the most crucial information, such as commuting time, and whether some minimum requirements were met, to be at the top of the explanation. The recruiters internally disagreed on how long the text should be (\textit{``it doesn't need to be brief. It's nice to have things to talk about"}, P3, Q5.2; \textit{``Three paragraphs of three sentences would be fine"} P4, Q5.1). One argument for having a longer text, was that it would provide the recruiter with more subject matter to discuss when trying to convince a candidate or company of the match's aptitude. On the other hand, having a shorter text that only focuses on the main arguments provided by the model would make it quicker for recruiters to compare different recommendations, after which they could come up with further arguments for the best one themselves. 

Although the recruiters managed to correctly interpret the graph, they found it to add little value for the most part (\textit{``I understand it, but it means little to me"}, P4, Q1.1). While they indicated that it could be useful in some specific scenarios, such as texts with a high level of complexity, or when having to support their final decision to a supervisor, they generally did not consider it to add any benefit compared to the textual explanation. Despite the fact that they developed a better understanding of the models' actual rationale, they doubted that they would use it much in their day-to-day tasks (\textit{``If I would have to use this for every vacancy, or every candidate, it would become a problem"}, P4, Q1.1). In the scenarios where they would consider it to be beneficial, they gravitated strongly toward the simplified version of the graph, considering the connections deemed unimportant by the model to be counterproductive in understanding the explanation. Only when a candidate or company would ask specifically about whether a specific skill, or past job, was taken into account, would they make use of the full version of the graph. 

Similarly to the candidates, the feature attribution chart in its current form did not assist the recruiters in correctly understanding the explanation. Again, though, did they indicate that a different type of bar chart could be useful in some scenarios. One use case for a bar chart the recruiters considered useful, was for it to be a central `hub' of sorts, where all possible vacancies for a candidate (and vice versa) were displayed, sorted by their matching score (\textit{``No, I would personally go for something like a top 10, for example"}, P3, Q2.1). This would allow the recruiters to quickly determine which potential matches are feasible enough to explore further. However, as an actual method of explaining the prediction, they indicated that the text, sometimes combined with the graph, would already be sufficient, causing the feature attribution chart to be largely irrelevant. 

% \begin{itemize}
%     \item Recruiters indicated a strong preference towards the textual explanations. While the exact examples were slightly unclear at first, and contained some redundant information to them, they considered them to be very promising nonetheless. They indicated a preference towards longer texts, that touch on a wide variety of topics (i.e., matches in terms of skill set, experience, interest, education, etc.), giving them more subject matter to talk about when trying to convince companies and candidates that they are indeed a good match for one another. However, they also indicated that detecting false positives should be as easy as possible, e.g., through being able to quickly cross-compare the requirements and wishes of the vacancy and candidate. Additionally, they strongly indicated that they would prefer to be able to manually verify the information provided by the model - the environment should therefore allow the recruiters to easily access the CV of the candidate, as well as the job vacancy text; preferably with highlighted text where the model found relevant similarities. 
%     \item While they did not dislike the graph explanations, they found them to be somewhat overwhelming - especially the full, non-simplified version. When parsing it for some time, and looking at the simplified view, they did eventually understand it and deemed it somewhat useful. They found it especially useful in case of doubt, as it gave them a more exhaustive overview of the model's actual rationale. However, they said they would only make use of it if the text itself did not suffice, and therefore found it more of an optional addition in general.
%     \item Similarly to the candidates, the bar chart in its current form contributed little to nothing towards their understanding of the model; however, they did indicate that with some tweaks, a bar chart of some sort could be useful to get an overview of the candidate/vacancy at a glance, or, alternatively, an overview of all the matches. They indicated that it would be useful to have a similar chart that sorted all the matches predicted by the model in order of their predicted matching score so that they could quickly sort through the large number of matches they have to deal with on a daily basis. Alternatively, they showed interest in having a bar chart that provides an overview of the most relevant skills or requirements (depending on the direction of the explanation). The addition of possibly relevant peers and different vacancies was perceived as nothing but confusing. 
% \end{itemize}

\subsubsection{Company representatives.}
As opposed to the candidates and recruiters, the company representatives were less positive about the textual explanation. Considering the company-side explanation contained a higher level of abstraction, it took the company representatives multiple iterations before they properly understood the explanation (\textit{``Now that I read it again, I see that it goes from person A, to B, to C."}, P5, Q3.2). They also found it difficult to take the explanation at face value, being wary of terms such as `relevant experience' - rather opting to manually verify whether the mentioned experience was actually relevant for the vacancy (\textit{``But judging how relevant the experience is for this vacancy, isn't possible based on this explanation"}, P5, Q1.2). Although a more detailed explanation could alleviate some of this hesitance, it would also lead to an even more complex explanation, possibly worsening the understandability further. 

On the other hand, the company representatives were considerably more positive about the graph explanation. Specifically, the simplified view of the graph allowed them to grasp the prediction at a glance. Where the textual explanation required some puzzling before the relations between different candidates, vacancies, and job types became clear, the company representatives quickly managed to detect the relevant relations in the graph (\textit{``This adds everything I need ... For me it's simply a matter of checking why the model made its decision - that being the line at the bottom, and that would be all"}, P6, Q2.1). As a result, the graph explanation also improved their trust in the model; one recruiter even mentioned that, given a high-enough performance of the model, they would simply use the simplified graph as a sanity check, fully trusting the model if the explanation seemed somewhat reasonable (\textit{``If the model does what it says it does, I would simply trust it"}, P6, Q3.2). 

The feature attribution map was again received poorly. The company representatives indicated that it did not help them get a better understanding of the model's reasoning compared to the graph (\textit{``It's usable, but it doesn't clarify why we ended up with the recommended candidate"}, P5, Q2.1). However, the company representatives did consider the feature attribution chart to be useful in a different scenario - to verify the model paid no attention to irrelevant details. With some tweaks, such as changing vacancy IDs into actual titles, the bar chart would allow company representatives to make sure the model did not pay attention to something that the company determined to be irrelevant to the position (e.g., a candidate possessing a specific skill, even though that skill is also taught during the onboarding process). Furthermore, an aggregated version of the feature attribution chart, which displays which types of features were considered most by the model, could help the company representatives parse the textual explanation more easily, allowing them to direct most of their attention to the more important information (\textit{``This is what I was looking for while reading the text. I tried to determine these values in my head, but I kept getting distracted"}, P5, Q2.1). However, as an actual explanation for the model's decision, they considered it too limited to draw any conclusions.

% \begin{itemize}
%     \item Text was considered a bit tricky to understand and required multiple iterations before it became clear what the actual message was. This was in part due to the somewhat cluttered writing style, and largely due to the relative complexity of the actual rationale. 
%     \item Pretty strong preference towards graphs, possibly due to more experience with reading similar visualizations during work.
%     \item Bar chart in its current form is too abstract (i.e., no value to knowing the importance of Vacancy 104819), but promising as an assistant while reading the text; especially the simple version --> what should I pay attention to while reading?
% \end{itemize}