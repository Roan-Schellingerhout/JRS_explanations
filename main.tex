% \documentclass[runningheads]{llncs}
\documentclass[runningheads]{llncs}

% \usepackage[a4paper, total={6in, 8in}]{geometry}
% \usepackage[backend=biber, maxcitenames=2, style = numeric-comp, citestyle=numeric]{biblatex}     
% \usepackage{titlesec}
\usepackage[hidelinks]{hyperref}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{booktabs}
% \usepackage[table,xcdraw, dvipsnames]{xcolor}
\usepackage{graphicx}
% \usepackage{ntheorem}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{natbib}
\usepackage{xcolor}
\usepackage[official]{eurosym}
\usepackage{lscape}
\usepackage{changepage}

\definecolor{xgreen}{HTML}{2bc253}
\definecolor{xyellow}{HTML}{fcba03}
\definecolor{xpink}{HTML}{f26884}

\DeclareCaptionType{Fig.}

\let\labelitemi\labelitemii

% \theoremseparator{:}
\newtheorem{hyp}{Hypothesis}

\setcounter{secnumdepth}{4}

\newcommand{\fb}[1]{\textcolor{red}{\textbf{Fra: #1}}}

% \titleformat{\paragraph}
% {\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
% \titlespacing*{\paragraph}
% {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\begin{document}

\title{A Co-design Study for Multi-Stakeholder Job Recommender System Explanations}
\titlerunning{Co-designing multi-stakeholder explanations}


\author{Anonymized}

%\author{Roan Schellingerhout \and Francesco Barile \and Nava Tintarev}

\date{April 2023}


%\institute{Department of Advanced Computing Sciences, Maastricht University, Maastricht, The Netherlands\\ \email{\{roan.schellingerhout,f.barile,n.tintarev\}}@maastrichtuniversity.nl}

\pagenumbering{arabic}



\maketitle

\begin{abstract}
    Recent legislation proposals have significantly increased the demand for eXplainable Artificial Intelligence (XAI) in many businesses, especially in so-called `high-risk' domains, such as recruitment. Within recruitment, AI has become commonplace, mainly in the form of job recommender systems (JRSs), which try to match candidates to vacancies, and vice versa. However, common XAI techniques often fall short in this domain due to the different levels and types of expertise of the individuals involved, making explanations difficult to generalize. To determine the explanation preferences of the different stakeholder types - candidates, recruiters, and companies - we created and validated a semi-structured interview guide. Using grounded theory, we structurally analyzed the results of these interviews and found that different stakeholder types indeed have strongly differing explanation preferences. \textit{Candidates} indicated a preference for brief, textual explanations that allow them to quickly judge potential matches. On the other hand, \textit{hiring managers} preferred visual graph-based explanations that provide a more technical and comprehensive overview at a glance. \textit{Recruiters} found more exhaustive textual explanations preferable, as those provided them with more talking points to convince both parties of the match. Based on these findings, we describe guidelines on how to design an explanation interface that fulfills the requirements of all three stakeholder types. Furthermore, we provide the validated interview guide, which can assist future research in determining the explanation preferences of different stakeholder types. 
\end{abstract}

\keywords{Explainable AI, Job Recommender Systems, User Studies, Grounded Theory}

\input{sections/1. introduction}
\input{sections/2. related work}
\input{sections/3. methodology}
\input{sections/4. results}
\input{sections/5. conclusion}

\bibliography{sources}
% \bibliographystyle{plainnat}
\bibliographystyle{splncs04}

\input{sections/6. appendix}

\end{document}
